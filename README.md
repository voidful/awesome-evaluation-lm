# awesome-evaluation-lm
Collection Of Automated Language Model Assessment

| Project Name                | GitHub Link                                                                                           | Description                                                                                     |
|-----------------------------|-------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------|
| PandaLM                     | [PandaLM](https://github.com/WeOpenML/PandaLM)                                                        | A universal evaluation framework designed to assess both foundation models and chat models.     |
| MiniCheck                   | [MiniCheck](https://github.com/Liyan06/MiniCheck)                                                     | A toolkit for evaluating the quality of model outputs, with a focus on assessing factual consistency. |
| ChatEval                    | [ChatEval](https://github.com/thunlp/ChatEval)                                                        | An evaluation framework designed specifically for chatbots, including both automatic and human evaluation methods. |
| auto-j                      | [auto-j](https://github.com/GAIR-NLP/auto-j)                                                          | A tool for automatically evaluating the fluency and grammatical correctness of text generated by language models. |
| LLMBar                      | [LLMBar](https://github.com/princeton-nlp/LLMBar)                                                     | An evaluation framework that includes a large collection of benchmark tests for evaluating the performance of large language models on various tasks. |
| JudgeLM                     | [JudgeLM](https://github.com/baaivision/JudgeLM)                                                      | A platform for evaluating and comparing different language models.                               |
| LAMM                        | [LAMM](https://github.com/OpenGVLab/LAMM)                                                             | A tool that focuses on evaluating the factual consistency of models, providing test datasets and evaluation metrics. |
| Prometheus                  | [Prometheus](https://github.com/kaistAI/Prometheus)                                                   | An evaluation framework that includes test datasets and metrics for evaluating the performance of models on question answering tasks. |
| PCRM                        | [PCRM](https://github.com/wangclnlp/DeepSpeed-Chat-Extension/tree/PCRM)                               | A Prompt-based Chat Model evaluation method that provides evaluation metrics and code.          |
| TIGERScore                  | [TIGERScore](https://tiger-ai-lab.github.io/TIGERScore/)                                              | An evaluation framework that focuses on assessing the fluency and coherence of text generated by models. |

